{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blog post should include:\n",
    "\n",
    "The hook. Why study this collection? What makes it interesting? What research question are you asking?\n",
    "\n",
    "Methods and metrics. Rather than a formal academic paper treatment (\"We used RoBERTa Large with XY parameters...\"), I'm expecting a more informal but also descriptive treatment of what models or metrics you used, why you used them, and how they work. Showing off examples for how to compute a metric or what inputs/outputs look like can help here, as well as making diagrams of pieces of the process.\n",
    "\n",
    "Code snippets. Include code showing key steps in your process (if not the whole process). You don't have to annotate every piece of these in detail, but try to make the code at least somewhat human-friendly. If you're going the Jupyter Notebook route, the code + uploading your data files should be sufficient to reproduce all your results, there may be some slower processing that you prefer to do in advance; just provide a link in the notebook to where any other resources (code, processed versions of data files, etc.) live if you'd prefer to omit those. If you're taking a more prose-based approach, it may make more sense to just include a few key passages that you explain more, but I'd like to see at least three different \"interesting\" pieces of code: e.g. doing text processing, running models, or computing metrics.\n",
    "\n",
    "Results. What did you find out? How confident are you of these results? It's a good idea to mix aggregate statistics/findings with specific examples of documents that match (or are outliers of) a particular trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Language is complex. Just a few words can convey intense feelings. Some may argue that Twitter tweets are toxic. Is that really the case? Can we train machine learning models to predict the overall sentiment of just a few words in a tweet? \n",
    "\n",
    "We are studying a dataset of [Sentiment with 1.6 million tweets with locations](https://www.kaggle.com/datasets/vivekchary/sentiment-with-16-million-tweets-with-locations) found on Kaggle. This dataset consists of 1.6 million tweets from 2009-2017. A majority of these tweets are in English, and they originate from 33 different countries (approximately evenly distributed within the dataset). Each tweet also has a labeled sentiment score, with 0 = negative, 2 = neutral, and 4 = positive.\n",
    "\n",
    "Our research question consists of two parts: Can we accurately predict the sentiment of a tweet, and does the location of a tweet influence sentiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods and Metrics\n",
    "\n",
    "Before getting into the models, we found that 1.6 million tweets is a lot to parse. We chose to use a sample of this dataset by shuffling (for randomness) and splitting (using 5 parts) to get a smaller dataset of 320,000 tweets. \n",
    "\n",
    "We tested three different models on this smaller dataset: a TextBlob Naive Bayes model, a Natural Language ToolKit pre-trained analyzer using VADER, and a Hugging Face Transformer using BERTweet (a RoBERTa model trained on English tweets.)\n",
    "\n",
    "First, import all functions in `helper_functions.py` by running the code below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how we processed our 1.6 million tweets into a managable chunk. Note that we drop a few unnecessary column titles like id, date, flag, and user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1333545</th>\n",
       "      <td>4</td>\n",
       "      <td>@WalkingHorse Good Morning! Thanks for the info</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985391</th>\n",
       "      <td>4</td>\n",
       "      <td>@fostress i like the word polopoly</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044360</th>\n",
       "      <td>4</td>\n",
       "      <td>In case you ain't seen the new Dr. Pepper/Dr. ...</td>\n",
       "      <td>Jamaica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613122</th>\n",
       "      <td>0</td>\n",
       "      <td>I dont want to go home</td>\n",
       "      <td>Dominican Republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40766</th>\n",
       "      <td>0</td>\n",
       "      <td>making PowerP -&amp;gt; last minute</td>\n",
       "      <td>Ecuador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202892</th>\n",
       "      <td>0</td>\n",
       "      <td>@DJWebstar are u still mad at me again homie!!...</td>\n",
       "      <td>Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103245</th>\n",
       "      <td>4</td>\n",
       "      <td>is happy with my GHWT band set.... just love t...</td>\n",
       "      <td>South Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893008</th>\n",
       "      <td>4</td>\n",
       "      <td>eatin chicharones, talkin 2 vale, thinkin bout...</td>\n",
       "      <td>Indonesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379744</th>\n",
       "      <td>0</td>\n",
       "      <td>@iam_Casper82 too bad! i'll try to check Anoop...</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401277</th>\n",
       "      <td>0</td>\n",
       "      <td>On my way home.. Dammit that rope fucked up my...</td>\n",
       "      <td>Tanzania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text  \\\n",
       "1333545       4   @WalkingHorse Good Morning! Thanks for the info    \n",
       "985391        4                @fostress i like the word polopoly    \n",
       "1044360       4  In case you ain't seen the new Dr. Pepper/Dr. ...   \n",
       "613122        0                            I dont want to go home    \n",
       "40766         0                   making PowerP -&gt; last minute    \n",
       "...         ...                                                ...   \n",
       "202892        0  @DJWebstar are u still mad at me again homie!!...   \n",
       "1103245       4  is happy with my GHWT band set.... just love t...   \n",
       "893008        4  eatin chicharones, talkin 2 vale, thinkin bout...   \n",
       "379744        0  @iam_Casper82 too bad! i'll try to check Anoop...   \n",
       "401277        0  On my way home.. Dammit that rope fucked up my...   \n",
       "\n",
       "                   location  \n",
       "1333545               China  \n",
       "985391               Russia  \n",
       "1044360             Jamaica  \n",
       "613122   Dominican Republic  \n",
       "40766               Ecuador  \n",
       "...                     ...  \n",
       "202892              Nigeria  \n",
       "1103245        South Africa  \n",
       "893008            Indonesia  \n",
       "379744                   UK  \n",
       "401277             Tanzania  \n",
       "\n",
       "[320000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our downloaded data file is in the folder data/\n",
    "df = pd.read_csv(\"data/sentiment140_with_location.csv\", names=['target','id','date','flag','user','text', 'location']) \n",
    "shuffled = df.sample(frac = 1)\n",
    "result = np.array_split(shuffled, 5)\n",
    "split_df = result[0]\n",
    "split_df = split_df.drop(['id', 'date', 'flag', 'user'], axis=1)\n",
    "split_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "The first model we used was a Naive Bayes classifier from the Textblob library. The Naive Bayes classifier is a supervised machine learning binary classifier that is based on probability. To use this classifier, we trained the model on a portion of our dataset for a given country. This entailed getting all of the tweets for a country, splitting our dataset in a train test split, which was about 80% train and 20% test. Each example used in training has a \"pos\" or \"neg\" label along with the tweet text. After training, we give our model an example from the test set, have it predict the sentiment based on the text, and then we compare the predicted sentiment to the actual sentiment label. We wanted to use the Naive Bayes classifier because we wanted to train our own model and learned about it in class. Note that each classifier we train is based on tweets and sentiment for a given country. Below is the code to train and test the classifier for tweets in the USA.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7417864476386037\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "dataset = getLocationTweetsWithSentiment(split_df, \"USA\")\n",
    "train = dataset[:int(0.8*len(dataset))] # change this so that its actually a train test split \n",
    "test = dataset[int(0.8*len(dataset)):]\n",
    "\n",
    "cl = NaiveBayesClassifier(train) # train needs tuples of text and pos/neg\n",
    "correct = 0\n",
    "for text, sentiment in test:\n",
    "    pred = cl.classify(text)\n",
    "    correct += (pred == sentiment)\n",
    "print(correct/len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code above, we can see that the Naive Bayes classifier that is trained on our data got an accuracy of about 74%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Sentiment Intensity Analyzer\n",
    "\n",
    "The second model that we used was the [Natural Language ToolKit Sentiment Intensity Analyzer](https://realpython.com/python-nltk-sentiment-analysis/). This model is a pre-trained sentiment analyzer using VADER (Valence Aware Dictionary and sEntiment Reasoner). We chose this model because we wanted to see how accurate a pre-trained sentiment analyzer would be on our dataset. This pre-trained analyzer also works best on small snippets of text from social media, like tweets! To use this model, run the following commands. First, usenload necessary resources for the SIA model: pip to install NLTK in your terminal:\n",
    "dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1534887517.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    python3 -m pip install nltk\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "nltk.download([\n",
    "    \"names\",\n",
    "    \"stopwords\",\n",
    "    \"state_union\",\n",
    "    \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\",\n",
    "    \"punkt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a SentimentIntensityAnalyzer and find sentiment scores for text. We looped through the tweets in `test` and used the polarity_scores() function to find sentiment scores. \n",
    "\n",
    "polarity_scores() returns a dictionary with the keys `pos` (positive score), `neg` (negative score), `neu` (neutral score), and `compound` (compounded score). The scores of `pos`, `neg`, and `neu` add up to 1, and `compound` is a combination of the previous keys such that it equals 0 if the overall score is neutral, <0 if the overall score is negative, and >0 if the overall score is positive.\n",
    "\n",
    "In the following code snippet, We use the value of `compound` to determine the sentiment score and compare this with the actual labeled score to get the accuracy of this model. Since none of the tweets in our dataset have a neutral score, we change all `neu` predictions by the model to `pos` predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\n",
      "[nltk_data]     certificate verify failed: unable to get local issuer\n",
      "[nltk_data]     certificate (_ssl.c:997)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "correct = 0\n",
    "\n",
    "for text, sentiment in test:\n",
    "    sia_pred = sia.polarity_scores(text)\n",
    "    if sia_pred[\"compound\"] < 0:\n",
    "        pred = \"neg\"\n",
    "    else:\n",
    "        pred = \"pos\"\n",
    "    correct += (pred == sentiment)\n",
    "\n",
    "print(\"SIA accuracy:\", correct/len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our SIA model performs worse than our Naive Bayes model. \n",
    "\n",
    "(reasoning why)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformer\n",
    "\n",
    "The final model that we looked at was a Hugging Face Transformer built off of [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets. This model is trained with SemEval 2017 corpus, which includes around 40,000 tweets. We chose to use this model because it was built specifically for tweets, and we wanted to see if another pre-trained model would yield similar results as our NLTK SIA model. This model takes in an array of strings to analyze and returns an array of dictionaries of the same length. Each dictionary has exactly two items, a `label` that signifies the sentiment and can equal `POS`, `NEG`, or `NEU`, and a `score` that represents the certainty probability of the label. To use this model, run the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "# get array of strings to analyze\n",
    "data = [text for text, sentiment in test]\n",
    "\n",
    "# store results of model\n",
    "results = specific_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we interate through results to find the accuracy of the model. Note that we also convert `NEU` scores to `pos` scores in this model since our dataset does not have any neutral labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "for i in range(len(results)):\n",
    "    pred = results[i]['label'].lower()\n",
    "    sentiment = test[i][1]\n",
    "    if pred == 'neu':\n",
    "        pred = \"pos\"\n",
    "    correct += (pred == sentiment)\n",
    "\n",
    "print(\"HF accuracy:\", correct/len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explanation of hf results + connect to previous model\n",
    "\n",
    "talk about neutral labels affecting accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the results suggest that Naive Bayes is the most accurate model, with roughly 70% accuracy. SIA and hugging face got roughly 60% accuracy. (JULIA INSERT EXPLANATIONS ON THE NEUTRAL AND MISLABELING OF STUFF)\n",
    "\n",
    "Below we show three different graphs, each representing the average predicted sentiment for four countries' tweets, using the classifiers. The first uses the Naive Bayes classifier and we can compare the actual sentiment to the predicted sentiment for USA, China, Spain, and Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"USA\", \"China\", \"Spain\", \"Canada\"]\n",
    "data = getSortedLocationScores(split_df)\n",
    "\n",
    "actual_sentiments = {\n",
    "    'USA': data[\"USA\"],\n",
    "    'China': data[\"China\"],\n",
    "    'Spain': data[\"Spain\"],\n",
    "    'Canada': data[\"Canada\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "predicted_sentiments_nb = {\n",
    "    'USA': averageSentimentByLocation(\"USA\", split_df),\n",
    "    'China': averageSentimentByLocation(\"China\", split_df),\n",
    "    'Spain': averageSentimentByLocation(\"Spain\", split_df),\n",
    "    'Canada': averageSentimentByLocation(\"Canada\", split_df)\n",
    "}\n",
    "\n",
    "plot_grouped_bar(locations, actual_sentiments, predicted_sentiments_nb, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we see how Naive Bayes compares to the actual average sentiment of the locations from the dataset. It appears that the predicted sentiment lean more towards an average of less than 2, suggesting that more of its predictions are negative rather than positive. However, it is clear that most of the results for the average sentiment for these locations is generally neutral, and is quite clse to the actual average sentiment. Since this model is trained on the dataset, this result is not very surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Intensity Analyzer\n",
    "predicted_sentiments_sia = {\n",
    "    'USA': averageSentimentByLocation_sia(\"USA\", split_df),\n",
    "    'China': averageSentimentByLocation_sia(\"China\", split_df),\n",
    "    'Spain': averageSentimentByLocation_sia(\"Spain\", split_df),\n",
    "    'Canada': averageSentimentByLocation_sia(\"Canada\", split_df)\n",
    "}\n",
    "\n",
    "plot_grouped_bar(locations, actual_sentiments, predicted_sentiments_sia, \"SIA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Transformer\n",
    "predicted_sentiments_sia = {\n",
    "    'USA': averageSentimentByLocation_hf(\"USA\", split_df),\n",
    "    'China': averageSentimentByLocation_hf(\"China\", split_df),\n",
    "    'Spain': averageSentimentByLocation_hf(\"Spain\", split_df),\n",
    "    'Canada': averageSentimentByLocation_hf(\"Canada\", split_df)\n",
    "}\n",
    "\n",
    "plot_grouped_bar(locations, actual_sentiments, predicted_sentiments_sia, \"Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- https://www.kaggle.com/datasets/vivekchary/sentiment-with-16-million-tweets-with-locations\n",
    "- https://textblob.readthedocs.io/en/dev/api_reference.html#module-textblob.en.sentiments\n",
    "- https://realpython.com/python-nltk-sentiment-analysis/\n",
    "- https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
