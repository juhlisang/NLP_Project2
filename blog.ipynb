{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blog post should include:\n",
    "\n",
    "The hook. Why study this collection? What makes it interesting? What research question are you asking?\n",
    "\n",
    "Methods and metrics. Rather than a formal academic paper treatment (\"We used RoBERTa Large with XY parameters...\"), I'm expecting a more informal but also descriptive treatment of what models or metrics you used, why you used them, and how they work. Showing off examples for how to compute a metric or what inputs/outputs look like can help here, as well as making diagrams of pieces of the process.\n",
    "\n",
    "Code snippets. Include code showing key steps in your process (if not the whole process). You don't have to annotate every piece of these in detail, but try to make the code at least somewhat human-friendly. If you're going the Jupyter Notebook route, the code + uploading your data files should be sufficient to reproduce all your results, there may be some slower processing that you prefer to do in advance; just provide a link in the notebook to where any other resources (code, processed versions of data files, etc.) live if you'd prefer to omit those. If you're taking a more prose-based approach, it may make more sense to just include a few key passages that you explain more, but I'd like to see at least three different \"interesting\" pieces of code: e.g. doing text processing, running models, or computing metrics.\n",
    "\n",
    "Results. What did you find out? How confident are you of these results? It's a good idea to mix aggregate statistics/findings with specific examples of documents that match (or are outliers of) a particular trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Language is complex. Just a few words can convey intense feelings.  < more hook stuff >\n",
    "\n",
    "We are studying a dataset of [Sentiment with 1.6 million tweets with locations](https://www.kaggle.com/datasets/vivekchary/sentiment-with-16-million-tweets-with-locations) found on Kaggle. This dataset consists of 1.6 million tweets from 2009-2017. A majority of these tweets are in English, and they originate from 33 different countries (approximately evenly distributed within the dataset). Each tweet also has a labeled sentiment score, with 0 = negative, 2 = neutral, and 4 = positive.\n",
    "\n",
    "Our research question consists of two parts: Can we accurately predict the sentiment of a tweet, and does the location of a tweet influence sentiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods and Metrics\n",
    "\n",
    "Methods and metrics. Rather than a formal academic paper treatment (\"We used RoBERTa Large with XY parameters...\"), I'm expecting a more informal but also descriptive treatment of what models or metrics you used, why you used them, and how they work. Showing off examples for how to compute a metric or what inputs/outputs look like can help here, as well as making diagrams of pieces of the process.\n",
    "\n",
    "Before getting into the models, we found that 1.6 million tweets is a lot to parse. We chose to use a sample of this dataset by shuffling (for randomness) and splitting (using 5 parts) to get a smaller dataset of 320,000 tweets. \n",
    "\n",
    "We tested three different models on this smaller dataset: a TextBlob Naive Bayes model, a Natural Language ToolKit pre-trained analyzer using VADER, and a Hugging Face Transformer using BERTweet (a RoBERTa model trained on English tweets.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "The TextBlob Naive Bayes model ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Sentiment Intensity Analyzer\n",
    "\n",
    "The second model that we used was the [Natural Language ToolKit Sentiment Intensity Analyzer](https://realpython.com/python-nltk-sentiment-analysis/). This model is a pre-trained sentiment analyzer using \n",
    "\n",
    "NLTK already has a built-in, pretrained sentiment analyzer called VADER (Valence Aware Dictionary and sEntiment Reasoner)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- https://www.kaggle.com/datasets/vivekchary/sentiment-with-16-million-tweets-with-locations\n",
    "- https://textblob.readthedocs.io/en/dev/api_reference.html#module-textblob.en.sentiments\n",
    "- https://realpython.com/python-nltk-sentiment-analysis/\n",
    "- https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
